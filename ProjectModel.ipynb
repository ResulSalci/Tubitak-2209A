{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYO44FXwr3du"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwRA2YK51YRP",
        "outputId": "ee507e05-654f-472d-ff74-10ebc66c8a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPehx8-AD6FW"
      },
      "outputs": [],
      "source": [
        "def string_to_tensor(str_to_convert):\n",
        "\n",
        "  char_set = \"abcçdefgğhıijklmnoöprsştuüvyz \"\n",
        "\n",
        "  y_list = []\n",
        "\n",
        "  for char in str_to_convert:\n",
        "    y_list.append(char_set.index(char))\n",
        "\n",
        "  tensor = tf.convert_to_tensor(y_list)\n",
        "\n",
        "  return tf.reshape(tensor, shape=(1, 1, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtSXlC1lefzA"
      },
      "outputs": [],
      "source": [
        "def tensor_to_string(tensor_to_convert):\n",
        "\n",
        "  char_set = \"abcçdefgğhıijklmnoöprsştuüvyz \"\n",
        "\n",
        "  result = \"\"\n",
        "\n",
        "  for timestep in tensor_to_convert[0]:\n",
        "\n",
        "    char_index = tf.argmax(timestep, axis=0)\n",
        "\n",
        "    if(char_index != 30):\n",
        "      result += char_set[char_index]\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MBAJzpSJHBt"
      },
      "outputs": [],
      "source": [
        "def filter_non_chars(output_tensor, max_len):\n",
        "\n",
        "  filtered_list = []\n",
        "\n",
        "  for timestep in output_tensor[0]:\n",
        "    if(tf.argmax(timestep, axis=0) != 30):\n",
        "      filtered_list.append(timestep)\n",
        "\n",
        "  result = tf.convert_to_tensor(filtered_list, dtype=tf.float32)\n",
        "  result = result[0:max_len:1]\n",
        "  result = tf.reshape(result, (1, max_len, len(output_tensor[0][0])))\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWDx4GJplHQP"
      },
      "outputs": [],
      "source": [
        "def read_input_csv(file_name):\n",
        "  x_train = pd.read_csv(os.getcwd() + f\"/drive/MyDrive/data/input/{file_name}\")\n",
        "  x_train = tf.reshape(x_train, shape=(1, x_train.shape[0], -1))\n",
        "\n",
        "  return x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhWZceEKGtd4"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "\n",
        "  x_train.append(read_input_csv(f\"{i}_audio.csv\"))\n",
        "\n",
        "  with open(os.getcwd() + f\"/drive/MyDrive/data/output/{i}_label.txt\", \"r\") as f:\n",
        "    y_train.append(string_to_tensor(f.readline()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UmuDAI1s0Ku",
        "outputId": "b4eef494-445a-4412-984f-bed62edb5d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, 32)]        0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 1024)        4329472   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 1024)        0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, None, 1024)        8392704   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 1024)        0         \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, None, 2048)        25174016  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, None, 2048)        0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 31)          63519     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37959711 (144.80 MB)\n",
            "Trainable params: 37959711 (144.80 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "def create_model() -> keras.Model:\n",
        "  inputs = keras.Input(shape=(None, 32))\n",
        "  x = layers.LSTM(1024, return_sequences=True, kernel_regularizer=keras.regularizers.L1(0.01))(inputs)\n",
        "  x = layers.Dropout(.1)(x)\n",
        "  x = layers.LSTM(1024, return_sequences=True, kernel_regularizer=keras.regularizers.L1(0.01))(x)\n",
        "  x = layers.Dropout(.11)(x)\n",
        "  x = layers.LSTM(2048, return_sequences=True, kernel_regularizer=keras.regularizers.L1(0.01))(x)\n",
        "  x = layers.Dropout(.1)(x)\n",
        "  outputs = layers.Dense(31, activation=\"softmax\")(x)\n",
        "\n",
        "  return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# x_train = tf.constant([[[1,20,3], [4, 11, 56], [4, 5, 7], [87, 10, 0]]], dtype=tf.float32)\n",
        "\n",
        "# y_real = tf.constant([[[11, 7]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHM29cSYZe2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c857015-7d29-47d8-f645-32028fa30fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f0c6c3c45e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f0c6c3c45e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 : loss = 3.0610334873199463\n",
            "Epoch 2 : loss = 3.077976703643799\n",
            "Epoch 3 : loss = 3.0357868671417236\n",
            "Epoch 4 : loss = 3.0282044410705566\n",
            "Epoch 5 : loss = 3.025486469268799\n",
            "Epoch 6 : loss = 3.012667179107666\n",
            "Epoch 7 : loss = 3.00700306892395\n",
            "Epoch 8 : loss = 3.0055713653564453\n",
            "Epoch 9 : loss = 3.0142390727996826\n",
            "Epoch 10 : loss = 3.0189292430877686\n",
            "Epoch 11 : loss = 3.013176441192627\n",
            "Epoch 12 : loss = 3.007023334503174\n",
            "Epoch 13 : loss = 2.9980320930480957\n",
            "Epoch 14 : loss = 2.9905247688293457\n",
            "Epoch 15 : loss = 3.0080766677856445\n",
            "Epoch 16 : loss = 3.01528263092041\n",
            "Epoch 17 : loss = 3.0198886394500732\n",
            "Epoch 18 : loss = 2.9914822578430176\n",
            "Epoch 19 : loss = 2.969522476196289\n",
            "Epoch 20 : loss = 2.9989712238311768\n",
            "Epoch 21 : loss = 2.9752888679504395\n",
            "Epoch 22 : loss = 2.951422929763794\n",
            "Epoch 23 : loss = 2.955664873123169\n",
            "Epoch 24 : loss = 2.9417574405670166\n",
            "Epoch 25 : loss = 2.9260799884796143\n",
            "Epoch 26 : loss = 2.9390547275543213\n",
            "Epoch 27 : loss = 2.923968553543091\n",
            "Epoch 28 : loss = 2.9243714809417725\n",
            "Epoch 29 : loss = 2.9077186584472656\n",
            "Epoch 30 : loss = 2.878006935119629\n",
            "Epoch 31 : loss = 2.9086575508117676\n",
            "Epoch 32 : loss = 2.8840978145599365\n",
            "Epoch 33 : loss = 2.8777406215667725\n",
            "Epoch 34 : loss = 2.845444917678833\n",
            "Epoch 35 : loss = 2.8632218837738037\n",
            "Epoch 36 : loss = 2.8214244842529297\n",
            "Epoch 37 : loss = 2.8077614307403564\n",
            "Epoch 38 : loss = 2.764711856842041\n",
            "Epoch 39 : loss = 2.6919124126434326\n",
            "Epoch 40 : loss = 2.6691269874572754\n",
            "Epoch 41 : loss = 2.67808198928833\n",
            "Epoch 42 : loss = 2.6172099113464355\n",
            "Epoch 43 : loss = 2.6241965293884277\n",
            "Epoch 44 : loss = 2.6573007106781006\n",
            "Epoch 45 : loss = 2.6613543033599854\n",
            "Epoch 46 : loss = 2.546666145324707\n",
            "Epoch 47 : loss = 2.574380397796631\n",
            "Epoch 48 : loss = 2.4957125186920166\n",
            "Epoch 49 : loss = 2.463233232498169\n",
            "Epoch 50 : loss = 2.455357313156128\n",
            "Epoch 51 : loss = 2.3839433193206787\n",
            "Epoch 52 : loss = 2.379723310470581\n",
            "Epoch 53 : loss = 2.3539159297943115\n",
            "Epoch 54 : loss = 2.2971768379211426\n",
            "Epoch 55 : loss = 2.309519052505493\n",
            "Epoch 56 : loss = 2.3929102420806885\n",
            "Epoch 57 : loss = 2.3301608562469482\n",
            "Epoch 58 : loss = 2.2190616130828857\n",
            "Epoch 59 : loss = 2.1937947273254395\n",
            "Epoch 60 : loss = 2.24556565284729\n",
            "Epoch 61 : loss = 2.2395548820495605\n",
            "Epoch 62 : loss = 2.1312763690948486\n",
            "Epoch 63 : loss = 2.1136772632598877\n",
            "Epoch 64 : loss = 2.0976898670196533\n",
            "Epoch 65 : loss = 2.0959632396698\n",
            "Epoch 66 : loss = 1.999621033668518\n",
            "Epoch 67 : loss = 1.9599895477294922\n",
            "Epoch 68 : loss = 1.993563175201416\n",
            "Epoch 69 : loss = 2.1043152809143066\n",
            "Epoch 70 : loss = 2.0816073417663574\n",
            "Epoch 71 : loss = 1.9705348014831543\n",
            "Epoch 72 : loss = 1.8836841583251953\n",
            "Epoch 73 : loss = 1.8842965364456177\n",
            "Epoch 74 : loss = 1.933289647102356\n",
            "Epoch 75 : loss = 1.851682424545288\n",
            "Epoch 76 : loss = 1.763347864151001\n",
            "Epoch 77 : loss = 1.7545233964920044\n",
            "Epoch 78 : loss = 1.848924160003662\n",
            "Epoch 79 : loss = 1.840441107749939\n",
            "Epoch 80 : loss = 1.7304946184158325\n",
            "Epoch 81 : loss = 1.6812269687652588\n",
            "Epoch 82 : loss = 1.6017436981201172\n",
            "Epoch 83 : loss = 1.5986964702606201\n",
            "Epoch 84 : loss = 1.5364619493484497\n",
            "Epoch 85 : loss = 1.5386477708816528\n",
            "Epoch 86 : loss = 1.6429229974746704\n",
            "Epoch 87 : loss = 1.6193474531173706\n",
            "Epoch 88 : loss = 1.6157625913619995\n",
            "Epoch 89 : loss = 1.7271939516067505\n",
            "Epoch 90 : loss = 1.686855673789978\n",
            "Epoch 91 : loss = 1.634342074394226\n",
            "Epoch 92 : loss = 1.5504359006881714\n",
            "Epoch 93 : loss = 1.475084900856018\n",
            "Epoch 94 : loss = 1.410372018814087\n",
            "Epoch 95 : loss = 1.3936891555786133\n",
            "Epoch 96 : loss = 1.398075819015503\n",
            "Epoch 97 : loss = 1.3538247346878052\n",
            "Epoch 98 : loss = 1.457900047302246\n",
            "Epoch 99 : loss = 1.6383438110351562\n",
            "Epoch 100 : loss = 1.4918112754821777\n",
            "Epoch 101 : loss = 1.3569296598434448\n",
            "Epoch 102 : loss = 1.3409377336502075\n",
            "Epoch 103 : loss = 1.2753825187683105\n",
            "Epoch 104 : loss = 1.275981068611145\n",
            "Epoch 105 : loss = 1.3806719779968262\n",
            "Epoch 106 : loss = 1.2530274391174316\n",
            "Epoch 107 : loss = 1.2271842956542969\n",
            "Epoch 108 : loss = 1.4211032390594482\n",
            "Epoch 109 : loss = 1.3763142824172974\n",
            "Epoch 110 : loss = 1.2638858556747437\n",
            "Epoch 111 : loss = 1.275268316268921\n",
            "Epoch 112 : loss = 1.3470523357391357\n",
            "Epoch 113 : loss = 1.2933475971221924\n",
            "Epoch 114 : loss = 1.2120473384857178\n",
            "Epoch 115 : loss = 1.1995046138763428\n",
            "Epoch 116 : loss = 1.1506035327911377\n",
            "Epoch 117 : loss = 1.0690685510635376\n",
            "Epoch 118 : loss = 1.0508065223693848\n",
            "Epoch 119 : loss = 1.0685678720474243\n",
            "Epoch 120 : loss = 1.1214113235473633\n",
            "Epoch 121 : loss = 1.1378920078277588\n",
            "Epoch 122 : loss = 1.176155686378479\n",
            "Epoch 123 : loss = 1.220176100730896\n",
            "Epoch 124 : loss = 1.1195094585418701\n",
            "Epoch 125 : loss = 1.0394432544708252\n",
            "Epoch 126 : loss = 1.0265363454818726\n",
            "Epoch 127 : loss = 1.0733890533447266\n",
            "Epoch 128 : loss = 1.0876487493515015\n",
            "Epoch 129 : loss = 0.9945853352546692\n",
            "Epoch 130 : loss = 1.0213149785995483\n",
            "Epoch 131 : loss = 1.1279984712600708\n",
            "Epoch 132 : loss = 1.1164911985397339\n",
            "Epoch 133 : loss = 0.970207154750824\n",
            "Epoch 134 : loss = 0.9328031539916992\n",
            "Epoch 135 : loss = 0.9676613211631775\n",
            "Epoch 136 : loss = 0.9836790561676025\n",
            "Epoch 137 : loss = 0.9238560795783997\n",
            "Epoch 138 : loss = 0.8802992701530457\n",
            "Epoch 139 : loss = 0.8692444562911987\n",
            "Epoch 140 : loss = 0.9003443121910095\n",
            "Epoch 141 : loss = 0.888648271560669\n",
            "Epoch 142 : loss = 0.8707094192504883\n",
            "Epoch 143 : loss = 0.8629187941551208\n",
            "Epoch 144 : loss = 0.8266244530677795\n",
            "Epoch 145 : loss = 0.809150755405426\n",
            "Epoch 146 : loss = 0.8232797980308533\n",
            "Epoch 147 : loss = 0.7858178019523621\n",
            "Epoch 148 : loss = 0.789351224899292\n",
            "Epoch 149 : loss = 0.9920173287391663\n",
            "Epoch 150 : loss = 0.8665183782577515\n",
            "Epoch 151 : loss = 0.7943762540817261\n",
            "Epoch 152 : loss = 0.9445627331733704\n",
            "Epoch 153 : loss = 0.9653958678245544\n",
            "Epoch 154 : loss = 0.9009674191474915\n",
            "Epoch 155 : loss = 0.8095130324363708\n",
            "Epoch 156 : loss = 0.8036500215530396\n",
            "Epoch 157 : loss = 0.7404367327690125\n",
            "Epoch 158 : loss = 0.6911131143569946\n",
            "Epoch 159 : loss = 0.6672891974449158\n",
            "Epoch 160 : loss = 0.6626394391059875\n",
            "Epoch 161 : loss = 0.6854732036590576\n",
            "Epoch 162 : loss = 0.6358271837234497\n",
            "Epoch 163 : loss = 0.670219898223877\n",
            "Epoch 164 : loss = 0.6905730366706848\n",
            "Epoch 165 : loss = 0.6340346932411194\n",
            "Epoch 166 : loss = 0.7499265670776367\n",
            "Epoch 167 : loss = 0.8687794208526611\n",
            "Epoch 168 : loss = 0.7402185797691345\n",
            "Epoch 169 : loss = 0.695809006690979\n",
            "Epoch 170 : loss = 0.7418789267539978\n",
            "Epoch 171 : loss = 0.7313252687454224\n",
            "Epoch 172 : loss = 0.707935631275177\n",
            "Epoch 173 : loss = 0.647972583770752\n",
            "Epoch 174 : loss = 0.6280146837234497\n",
            "Epoch 175 : loss = 0.5846665501594543\n",
            "Epoch 176 : loss = 0.5560845136642456\n",
            "Epoch 177 : loss = 0.535698413848877\n",
            "Epoch 178 : loss = 0.5558292865753174\n",
            "Epoch 179 : loss = 0.5324286222457886\n",
            "Epoch 180 : loss = 0.5323655605316162\n",
            "Epoch 181 : loss = 0.5116963982582092\n",
            "Epoch 182 : loss = 0.5258725881576538\n",
            "Epoch 183 : loss = 0.5391709208488464\n",
            "Epoch 184 : loss = 0.4930274784564972\n",
            "Epoch 185 : loss = 0.5220929384231567\n",
            "Epoch 186 : loss = 0.6288471221923828\n",
            "Epoch 187 : loss = 0.5686889290809631\n",
            "Epoch 188 : loss = 0.5059890151023865\n",
            "Epoch 189 : loss = 0.6006839275360107\n",
            "Epoch 190 : loss = 0.6495954990386963\n",
            "Epoch 191 : loss = 0.5637317895889282\n",
            "Epoch 192 : loss = 0.5750637650489807\n",
            "Epoch 193 : loss = 0.6668596267700195\n",
            "Epoch 194 : loss = 0.6592715978622437\n",
            "Epoch 195 : loss = 0.598866879940033\n",
            "Epoch 196 : loss = 0.5580024719238281\n",
            "Epoch 197 : loss = 0.5135787725448608\n",
            "Epoch 198 : loss = 0.4690037667751312\n",
            "Epoch 199 : loss = 0.48046305775642395\n",
            "Epoch 200 : loss = 0.520700991153717\n",
            "Epoch 201 : loss = 0.5334053635597229\n",
            "Epoch 202 : loss = 0.47862011194229126\n",
            "Epoch 203 : loss = 0.4116813838481903\n",
            "Epoch 204 : loss = 0.4097825586795807\n",
            "Epoch 205 : loss = 0.4094157814979553\n",
            "Epoch 206 : loss = 0.3996376395225525\n",
            "Epoch 207 : loss = 0.3947303593158722\n",
            "Epoch 208 : loss = 0.3860075771808624\n",
            "Epoch 209 : loss = 0.3718133568763733\n",
            "Epoch 210 : loss = 0.3758159577846527\n",
            "Epoch 211 : loss = 0.36736536026000977\n",
            "Epoch 212 : loss = 0.3571617901325226\n",
            "Epoch 213 : loss = 0.34869465231895447\n",
            "Epoch 214 : loss = 0.3567314147949219\n",
            "Epoch 215 : loss = 0.3487350046634674\n",
            "Epoch 216 : loss = 0.34758082032203674\n",
            "Epoch 217 : loss = 0.3505541980266571\n",
            "Epoch 218 : loss = 0.32352760434150696\n",
            "Epoch 219 : loss = 0.3170274496078491\n",
            "Epoch 220 : loss = 0.3353732228279114\n",
            "Epoch 221 : loss = 0.33250412344932556\n",
            "Epoch 222 : loss = 0.3320282995700836\n",
            "Epoch 223 : loss = 0.36678120493888855\n",
            "Epoch 224 : loss = 0.3802748918533325\n",
            "Epoch 225 : loss = 0.32803139090538025\n",
            "Epoch 226 : loss = 0.3508601188659668\n",
            "Epoch 227 : loss = 0.38072577118873596\n",
            "Epoch 228 : loss = 0.37288787961006165\n",
            "Epoch 229 : loss = 0.3391442894935608\n",
            "Epoch 230 : loss = 0.39953774213790894\n",
            "Epoch 231 : loss = 0.4386286735534668\n",
            "Epoch 232 : loss = 0.37539801001548767\n",
            "Epoch 233 : loss = 0.34185269474983215\n",
            "Epoch 234 : loss = 0.32157307863235474\n",
            "Epoch 235 : loss = 0.3253517746925354\n",
            "Epoch 236 : loss = 0.3019110858440399\n",
            "Epoch 237 : loss = 0.29130470752716064\n",
            "Epoch 238 : loss = 0.28271132707595825\n",
            "Epoch 239 : loss = 0.2768782675266266\n",
            "Epoch 240 : loss = 0.275986909866333\n",
            "Epoch 241 : loss = 0.26265931129455566\n",
            "Epoch 242 : loss = 0.2600244879722595\n",
            "Epoch 243 : loss = 0.25617244839668274\n",
            "Epoch 244 : loss = 0.24813513457775116\n",
            "Epoch 245 : loss = 0.24329033493995667\n",
            "Epoch 246 : loss = 0.2448953092098236\n",
            "Epoch 247 : loss = 0.2632823586463928\n",
            "Epoch 248 : loss = 0.27487650513648987\n",
            "Epoch 249 : loss = 0.25534436106681824\n",
            "Epoch 250 : loss = 0.2563181221485138\n",
            "Epoch 251 : loss = 0.26408979296684265\n",
            "Epoch 252 : loss = 0.27905911207199097\n",
            "Epoch 253 : loss = 0.24514718353748322\n",
            "Epoch 254 : loss = 0.22531993687152863\n",
            "Epoch 255 : loss = 0.22834573686122894\n",
            "Epoch 256 : loss = 0.21384067833423615\n",
            "Epoch 257 : loss = 0.21966920793056488\n",
            "Epoch 258 : loss = 0.21245287358760834\n",
            "Epoch 259 : loss = 0.20310541987419128\n",
            "Epoch 260 : loss = 0.20248481631278992\n",
            "Epoch 261 : loss = 0.19755618274211884\n",
            "Epoch 262 : loss = 0.20012441277503967\n",
            "Epoch 263 : loss = 0.18687668442726135\n",
            "Epoch 264 : loss = 0.18940359354019165\n",
            "Epoch 265 : loss = 0.18738780915737152\n",
            "Epoch 266 : loss = 0.18452507257461548\n",
            "Epoch 267 : loss = 0.18701766431331635\n",
            "Epoch 268 : loss = 0.19023500382900238\n",
            "Epoch 269 : loss = 0.19396105408668518\n",
            "Epoch 270 : loss = 0.1861645132303238\n",
            "Epoch 271 : loss = 0.18087545037269592\n",
            "Epoch 272 : loss = 0.17424152791500092\n",
            "Epoch 273 : loss = 0.18409940600395203\n",
            "Epoch 274 : loss = 0.17176757752895355\n",
            "Epoch 275 : loss = 0.17534767091274261\n",
            "Epoch 276 : loss = 0.1812562644481659\n",
            "Epoch 277 : loss = 0.17228254675865173\n",
            "Epoch 278 : loss = 0.16075120866298676\n",
            "Epoch 279 : loss = 0.17126794159412384\n",
            "Epoch 280 : loss = 0.19118501245975494\n",
            "Epoch 281 : loss = 0.20930419862270355\n",
            "Epoch 282 : loss = 0.19636130332946777\n",
            "Epoch 283 : loss = 0.18041475117206573\n",
            "Epoch 284 : loss = 0.16493608057498932\n",
            "Epoch 285 : loss = 0.16701483726501465\n",
            "Epoch 286 : loss = 0.15474219620227814\n",
            "Epoch 287 : loss = 0.1459936499595642\n",
            "Epoch 288 : loss = 0.14349211752414703\n",
            "Epoch 289 : loss = 0.14877437055110931\n",
            "Epoch 290 : loss = 0.14389356970787048\n",
            "Epoch 291 : loss = 0.1379578858613968\n",
            "Epoch 292 : loss = 0.1393568217754364\n",
            "Epoch 293 : loss = 0.13870258629322052\n",
            "Epoch 294 : loss = 0.13443683087825775\n",
            "Epoch 295 : loss = 0.1409413069486618\n",
            "Epoch 296 : loss = 0.1281919926404953\n",
            "Epoch 297 : loss = 0.12910889089107513\n",
            "Epoch 298 : loss = 0.14292089641094208\n",
            "Epoch 299 : loss = 0.1411944031715393\n",
            "Epoch 300 : loss = 0.13812896609306335\n"
          ]
        }
      ],
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "loss_func = tf.losses.SparseCategoricalCrossentropy()\n",
        "epochs = 300\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  for i in range(len(x_train)):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      y_pred = model(x_train[i], training=True)\n",
        "\n",
        "      loss = loss_func(y_train[i], filter_non_chars(y_pred, len(y_train[i][0][0])))\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "\n",
        "  print(f\"Epoch {epoch + 1} : loss = {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0fZqyH0Y1Uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e53f997a-e5d6-4395-9500-f5411c5b6f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "üsiiişşşşşııııııııııııaaaaaaaaşşşşşşşşşşıııııııııııııııııııııııııııışşşşşşşşşııııııııııışıııııııııııııııırrrrşşşşşşıııııııııışşşşşşşşşşşşşııııııırrraaaıııııııııııbbbbrrrşşaııııııııııııışşşşşşşşşıııbbbbbbraaaaaaııııııbbbbbbbrrrbbşşşşşşııııııııııııııııbbbbbbbbbbııııııııbbbbbbbaaaaaııııııııııırrrrrşşşşşşşşşııııııııııııbşşşaaaııııııbbbrrıııııııııııııırryyyyyyyyyybbbbşşşşrrrrrrşşııııııııııııbbbbrraaaaıııııııbbbbrrrrrrrbbbbbbşşaıııııırrrrrrrffşşşşşııııbbbbbbrrrrraaaııııışrrrrrrççççııııııııyyybbbbbbbbbbaaaaaaıııırrrrrrrrfbııbbbbbbbbrrrraaaaaaaybbbbbbbbbççaaaaııııbbbbffffffaaıııııbbbffrrrraaaa\n"
          ]
        }
      ],
      "source": [
        "x_test = read_input_csv(f\"{10}_audio.csv\")\n",
        "\n",
        "print(tensor_to_string(model.predict(x_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0raykvmL3UeK",
        "outputId": "140438d0-e447-4bb2-b083-87d53ef8360e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}